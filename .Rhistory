# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=1000)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.1000[i]=mis.error
}
mean(mis.errors.1000)
ave.errors=c(mean(mis.errors.50),mean(mis.errors.100),mean(mis.errors.500),mean(mis.errors.1000))
plot(c(50,100,500,1000),ave.errors, xlab = 'n trees', ylab='Average Error Rate', type='b')
title('Amount of Trees vs Error Rate')
ave.errors=c(mean(mis.errors.50),mean(mis.errors.100),mean(mis.errors.500),mean(mis.errors.1000))
plot(c(50,100,500,1000),ave.errors, xlab = 'n trees', ylab='Estimated Error Rate', type='b')
title('Amount of Trees vs Error Rate')
ave.errors=c(mean(mis.errors.50),mean(mis.errors.100),mean(mis.errors.500),mean(mis.errors.1000))
plot(c(50,100,500,1000),ave.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')
title('Amount of Trees vs Test Error Rate')
pairs(Iris)
summary(Iris)
pairs(Iris)
knitr::opts_chunk$set(echo = TRUE)
install.packages(c('randomForest', 'caret', 'class','ggplot2'))
library(class)
library(caret)
library(randomForest)
library(ggplot2)
library(MASS)
# load data from CSV
Iris <- read.csv("bezedekIris.csv")
View(Iris)
attach(Iris)
Iris$Class <- as.factor(Iris$Class) # for handling qualitative response
summary(Iris)
pairs(Iris)
cor(subset(Iris, select = -c(Class)))
set.seed(1)
k = 10
cv.error <- rep(0, k)
splits <- createFolds(Class, returnTrain = FALSE)
for(i in 1:k) {
#Split into train and test
test <- splits[[i]]
train <- -(test)
Iris.test <- Iris[test, ]
Class.test <- Class[test]
#Train model
lda.fit <- lda(Class ~ ., data = Iris, subset = train)
#Test model
lda.pred <- predict(lda.fit, Iris.test)
lda.class <- lda.pred$class
cv.error[i] <- mean(lda.class != Class.test)
}
cv.error
mean(cv.error)
lda.fit <- lda(Class ~ ., data = Iris)
lda.fit
set.seed(1)
k = 10
cv.error <- rep(0, k)
splits <- createFolds(Class, returnTrain = FALSE)
for(i in 1:k) {
#Split into train and test
test <- splits[[i]]
train <- -(test)
Iris.test <- Iris[test, ]
Class.test <- Class[test]
#Train model
qda.fit <- qda(Class ~ ., data = Iris, subset = train)
#Test model
qda.pred <- predict(qda.fit, Iris.test)
qda.class <- qda.pred$class
cv.error[i] <- mean(qda.class != Class.test)
}
cv.error
mean(cv.error)
qda.fit <- qda(Class ~ ., data = Iris)
qda.fit
#### FITTING A KNN CLASSIFIER MODEL FOR DIFFERENT VALUES OF K TO FIND THE BEST K WITH LEAST MISCLASSIFICATION RATE
set.seed(300)
train_indices = sample(1:nrow(Iris), 0.8 * nrow(Iris))
train_data=Iris[train_indices, ]
test_data=Iris[-train_indices, ]
predictors=names(train_data)[1:4]
response="Class"
correct_classification=numeric()
misclassification_rates=numeric()
for (k_value in 1:10) {
knn_predictions=knn(train = train_data[, predictors], test = test_data[, predictors], cl = train_data[, response], k = k_value)
confusion_matrix=table(Actual = test_data[, response], Predicted = knn_predictions)
correct_classification=sum(diag(confusion_matrix)) / sum(confusion_matrix)
misclassification_rate=1 - correct_classification
misclassification_rates=c(misclassification_rates, misclassification_rate)
print(k_value)
print(confusion_matrix)
}
print(misclassification_rates)
plot(1:10, misclassification_rates, type = "b", xlab = "K",pch=16,col="red", ylab = "Misclassification Rate", main = "Misclassification rate for k values")
### After fitting the knn model on different k it is observed that 1,3,4,8,9,10 gives the minimum misclassification rate. So the knn model with k =3 can be choosen for further validation
### APPLYING 10 FOLD CROSS VALIDATION FOR TESTING THE MODEL PREDICTIONS
set.seed(123)
predictors = names(Iris)[1:4]
response = "Class"
irirs_data_2 = Iris[,c(predictors,response),drop=FALSE]
k_folds = 10
folds = createFolds(irirs_data_2$Class,k=k_folds,list=TRUE,returnTrain = FALSE)
missclassification_rates = numeric()
for (fold_ind in folds){
fold = unlist(fold_ind)
train_data = irirs_data_2[-fold,]
test_data = irirs_data_2[fold,]
knn_predictions = knn(train = train_data[,predictors],test=test_data[,predictors],cl=train_data[,response],k=3)
confusion_matrix = table(Actual=test_data[,response],Predicted=knn_predictions)
correct_classification = sum(diag(confusion_matrix))/sum(confusion_matrix)
missclassification_rate = 1-correct_classification
missclassification_rates = c(missclassification_rates,missclassification_rate)
}
print(missclassification_rates)
plot(1:10, missclassification_rates, type = "b",pch =16,col="red", xlab = "K", ylab = "Misclassification Rate", main = "Misclassification rate for k folds")
average_missclassification = mean(missclassification_rates)
print(average_missclassification)
### After applying the k fold cross validation the average misclassification rate shows up to be 0.04
set.seed(1)
k=10
mis.errors.50=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=50)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.50[i]=mis.error
}
mean(mis.errors.50)
set.seed(1)
k=10
mis.errors.100=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=100)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.100[i]=mis.error
}
mean(mis.errors.100)
set.seed(1)
k=10
mis.errors.500=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=500)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.500[i]=mis.error
}
mean(mis.errors.500)
set.seed(1)
k=10
mis.errors.1000=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=1000)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.1000[i]=mis.error
}
mean(mis.errors.1000)
ave.errors=c(mean(mis.errors.50),mean(mis.errors.100),mean(mis.errors.500),mean(mis.errors.1000))
plot(c(50,100,500,1000),ave.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')
title('Amount of Trees vs Test Error Rate')
set.seed(1)
ntree.max=500
k=10
n.errors=rep(ntree.max)
for (n in 1:ntree.max){
k.errors=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=n)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
k.errors[i]=mis.error
}
n.errors[n]=mean(k.errors)
}
n.errors
plot(seq(1:ntree.max),n.errors)
plot(seq(1:ntree.max),n.errors,type='a')
plot(seq(1:ntree.max),n.errors,type='b')
plot(seq(1:ntree.max),n.errors,type='-')
plot(seq(1:ntree.max),n.errors,type='c')
plot(seq(1:ntree.max),n.errors,type='1')
plot(seq(1:ntree.max),n.errors,type='b')
set.seed(1)
ntree.max=300
k=10
n.errors=rep(ntree.max)
for (n in 1:ntree.max){
k.errors=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=n)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
k.errors[i]=mis.error
}
n.errors[n]=mean(k.errors)
}
set.seed(1)
ntree.max=100
k=10
n.errors=rep(ntree.max)
for (n in 1:ntree.max){
k.errors=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=n)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
k.errors[i]=mis.error
}
n.errors[n]=mean(k.errors)
}
n.errors
plot(seq(1:ntree.max),n.errors,type='b')
plot(seq(1:ntree.max),n.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')
title('Amount of Trees vs Test Error Rate')
error.min=which.min(n.errors)
plot(seq(1:ntree.max),n.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')
title('Amount of Trees vs Test Error Rate')
error.min=which.min(n.errors)
error.min
plot(seq(1:ntree.max),n.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')
title('Amount of Trees vs Test Error Rate')
error.min=which.min(n.errors)
error.min
plot(seq(1:ntree.max),n.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='l')
title('Amount of Trees vs Test Error Rate')
lda.fit
lda.fit
plot(Iris$Sepal.Length,Iris$Sepal.Width,col=Iris$Class)
lda.fit
plot(Iris$Sepal.Length,Iris$Sepal.Width,col=Iris$Class)
legend(legend = levels(Iris$Class))
lda.fit
plot(Iris$Sepal.Length,Iris$Sepal.Width,col=Iris$Class)
legend('topright',legend = levels(Iris$Class))
lda.fit
plot(Iris$Sepal.Length,Iris$Sepal.Width,col=Iris$Class)
legend('topright',legend = levels(Iris$Class))
abline(lda.fit$scaling[2, 1]/lda.fit$scaling[1, 1], -lda.fit$scaling[3, 1]/lda.fit$scaling[1, 1], col = "blue")
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
ntree.max=100
k=10
n.errors=rep(ntree.max) # store avg error for each model
# cross-validate each model with increasing ntree
for (n in 1:ntree.max){
k.errors=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=n)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
k.errors[i]=mis.error
}
n.errors[n]=mean(k.errors)
}
knitr::opts_chunk$set(echo = TRUE)
install.packages(c('randomForest', 'caret', 'class','ggplot2'))
library(class)
library(caret)
library(randomForest)
library(ggplot2)
library(MASS)
# load data from CSV
Iris <- read.csv("bezedekIris.csv")
View(Iris)
attach(Iris)
Iris$Class <- as.factor(Iris$Class) # for handling qualitative response
summary(Iris)
pairs(Iris)
cor(subset(Iris, select = -c(Class)))
set.seed(1)
k = 10
cv.error <- rep(0, k)
splits <- createFolds(Class, returnTrain = FALSE)
for(i in 1:k) {
#Split into train and test
test <- splits[[i]]
train <- -(test)
Iris.test <- Iris[test, ]
Class.test <- Class[test]
#Train model
lda.fit <- lda(Class ~ ., data = Iris, subset = train)
#Test model
lda.pred <- predict(lda.fit, Iris.test)
lda.class <- lda.pred$class
cv.error[i] <- mean(lda.class != Class.test)
}
cv.error
mean(cv.error)
lda.fit <- lda(Class ~ ., data = Iris)
lda.fit
set.seed(1)
k = 10
cv.error <- rep(0, k)
splits <- createFolds(Class, returnTrain = FALSE)
for(i in 1:k) {
#Split into train and test
test <- splits[[i]]
train <- -(test)
Iris.test <- Iris[test, ]
Class.test <- Class[test]
#Train model
qda.fit <- qda(Class ~ ., data = Iris, subset = train)
#Test model
qda.pred <- predict(qda.fit, Iris.test)
qda.class <- qda.pred$class
cv.error[i] <- mean(qda.class != Class.test)
}
cv.error
mean(cv.error)
qda.fit <- qda(Class ~ ., data = Iris)
qda.fit
set.seed(300)
train_indices = sample(1:nrow(Iris), 0.8 * nrow(Iris))
train_data=Iris[train_indices, ]
test_data=Iris[-train_indices, ]
predictors=names(train_data)[1:4]
response="Class"
correct_classification=numeric()
misclassification_rates=numeric()
for (k_value in 1:10) {
knn_predictions=knn(train = train_data[, predictors], test = test_data[, predictors], cl = train_data[, response], k = k_value)
confusion_matrix=table(Actual = test_data[, response], Predicted = knn_predictions)
correct_classification=sum(diag(confusion_matrix)) / sum(confusion_matrix)
misclassification_rate=1 - correct_classification
misclassification_rates=c(misclassification_rates, misclassification_rate)
sprintf("K = %s",k_value)
sprintf("The confusion matrix of K= %s is %s",k_value,confusion_matrix)
}
sprintf("The misclassification rate for different values of k is %s",paste(misclassification_rates,collapse = ","))
plot(1:10, misclassification_rates, type = "b", xlab = "K",pch=16,col="red", ylab = "Misclassification Rate", main = "Misclassification rate for different k ")
set.seed(123)
predictors = names(Iris)[1:4]
response = "Class"
irirs_data_2 = Iris[,c(predictors,response),drop=FALSE]
k_folds = 10
folds = createFolds(irirs_data_2$Class,k=k_folds,list=TRUE,returnTrain = FALSE)
missclassification_rates = numeric()
for (fold_ind in folds){
fold = unlist(fold_ind)
train_data = irirs_data_2[-fold,]
test_data = irirs_data_2[fold,]
knn_predictions = knn(train = train_data[,predictors],test=test_data[,predictors],cl=train_data[,response],k=3)
confusion_matrix = table(Actual=test_data[,response],Predicted=knn_predictions)
correct_classification = sum(diag(confusion_matrix))/sum(confusion_matrix)
missclassification_rate = 1-correct_classification
missclassification_rates = c(missclassification_rates,missclassification_rate)
}
sprintf("The misclassification rate for k folds are: %s",paste(missclassification_rates,collapse = ","))
plot(1:10, missclassification_rates, type = "b",pch =16,col="red", xlab = "K", ylab = "Misclassification Rate", main = "Misclassification rate for k folds")
average_missclassification = mean(missclassification_rates)
sprintf("The average misclassification rate after validation is %s",average_missclassification)
set.seed(1)
k=10
mis.errors.50=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=50)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.50[i]=mis.error
}
mean(mis.errors.50)
set.seed(1)
k=10
mis.errors.100=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=100)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.100[i]=mis.error
}
mean(mis.errors.100)
set.seed(1)
k=10
mis.errors.500=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=500)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.500[i]=mis.error
}
mean(mis.errors.500)
set.seed(1)
k=10
mis.errors.1000=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=1000)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
mis.errors.1000[i]=mis.error
}
mean(mis.errors.1000)
ave.errors=c(mean(mis.errors.50),mean(mis.errors.100),mean(mis.errors.500),mean(mis.errors.1000))
plot(c(50,100,500,1000),ave.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')
title('Amount of Trees vs Test Error Rate')
set.seed(1)
ntree.max=100
k=10
n.errors=rep(ntree.max) # store avg error for each model
# cross-validate each model with increasing ntree
for (n in 1:ntree.max){
k.errors=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
for (i in 1:k){
# split into train/test
Iris.test=Iris[Iris.splits[[i]],]
Iris.train=Iris[-Iris.splits[[i]],]
# train model
rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=n)
# predict on test and get error from confusion matrix
predictions=predict(rf.fit, Iris.test)
rf.cm=table(predictions,Iris.test$Class)
mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
k.errors[i]=mis.error
}
n.errors[n]=mean(k.errors)
}
error.min=which.min(n.errors)
error.min
plot(seq(1:ntree.max),n.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='l')
title('Amount of Trees vs Test Error Rate')
