---
title: "EAS508 Project 2"
author: "Nicholas Cruz, Sean Grzenda, Priya Patil"
date: "2023-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages(c('randomForest', 'caret', 'class','ggplot2'))

library(class)
library(caret)
library(randomForest)
library(ggplot2)
library(MASS)

# load data from CSV
Iris <- read.csv("bezedekIris.csv")
View(Iris)
attach(Iris)

Iris$Class <- as.factor(Iris$Class) # for handling qualitative response
```

## Summary of Dataset

We will be using the [Iris Dataset](https://archive.ics.uci.edu/dataset/53/iris), which contains various measurements of setosa, versicolor, and virginica flowers. The goal is to build a classification model predicting flower class that minimizes test misclassification rate. For each model (LDA, QDA, KNN, and Random Forest), we will tune hyperparameters where appropriate and estimate test error using 10-fold CV.

**Description**

```{r}
summary(Iris)
```

**Scatterplots**

```{r}
pairs(Iris)
```

**Correlation Matrix**

```{r}
cor(subset(Iris, select = -c(Class)))
```

## LDA

```{r}
set.seed(1)
k = 10
cv.error <- rep(0, k)
splits <- createFolds(Class, returnTrain = FALSE)
for(i in 1:k) {
  #Split into train and test
  test <- splits[[i]]
  train <- -(test)
  Iris.test <- Iris[test, ]
  Class.test <- Class[test]
  
  #Train model
  lda.fit <- lda(Class ~ ., data = Iris, subset = train)
  
  #Test model
  lda.pred <- predict(lda.fit, Iris.test)
  lda.class <- lda.pred$class
  cv.error[i] <- mean(lda.class != Class.test)
}
cv.error
mean(cv.error)

lda.fit <- lda(Class ~ ., data = Iris)
lda.fit
```

## QDA

```{r}
set.seed(1)
k = 10
cv.error <- rep(0, k)
splits <- createFolds(Class, returnTrain = FALSE)
for(i in 1:k) {
  #Split into train and test
  test <- splits[[i]]
  train <- -(test)
  Iris.test <- Iris[test, ]
  Class.test <- Class[test]
  
  #Train model
  qda.fit <- qda(Class ~ ., data = Iris, subset = train)
  
  #Test model
  qda.pred <- predict(qda.fit, Iris.test)
  qda.class <- qda.pred$class
  cv.error[i] <- mean(qda.class != Class.test)
}
cv.error
mean(cv.error)

qda.fit <- qda(Class ~ ., data = Iris)
qda.fit
```

## KNN

FITTING A KNN CLASSIFIER MODEL FOR DIFFERENT VALUES OF K TO FIND THE BEST K WITH LEAST MISCLASSIFICATION RATE

```{r}
set.seed(300)
train_indices = sample(1:nrow(Iris), 0.8 * nrow(Iris))
train_data=Iris[train_indices, ]
test_data=Iris[-train_indices, ]
predictors=names(train_data)[1:4]
response="Class"
correct_classification=numeric()
misclassification_rates=numeric()
for (k_value in 1:10) {
  knn_predictions=knn(train = train_data[, predictors], test = test_data[, predictors], cl = train_data[, response], k = k_value)
  confusion_matrix=table(Actual = test_data[, response], Predicted = knn_predictions)
  correct_classification=sum(diag(confusion_matrix)) / sum(confusion_matrix)
  misclassification_rate=1 - correct_classification
  misclassification_rates=c(misclassification_rates, misclassification_rate)
  sprintf("K = %s",k_value)
  sprintf("The confusion matrix of K= %s is %s",k_value,confusion_matrix)
  }
sprintf("The misclassification rate for different values of k is %s",paste(misclassification_rates,collapse = ","))
plot(1:10, misclassification_rates, type = "b", xlab = "K",pch=16,col="red", ylab = "Misclassification Rate", main = "Misclassification rate for different k ")
```

After fitting the KNN model for different values of k the best knn model for this data comes out to be k=4. So the model for further validation is a KNN model with k=4

APPLYING 10 FOLD CROSS VALIDATION FOR TESTING THE MODEL PREDICTIONS

```{r}
set.seed(123)
predictors = names(Iris)[1:4]
response = "Class"
irirs_data_2 = Iris[,c(predictors,response),drop=FALSE]
k_folds = 10
folds = createFolds(irirs_data_2$Class,k=k_folds,list=TRUE,returnTrain = FALSE)
missclassification_rates = numeric()
for (fold_ind in folds){
  fold = unlist(fold_ind)
  train_data = irirs_data_2[-fold,]
  test_data = irirs_data_2[fold,]
  knn_predictions = knn(train = train_data[,predictors],test=test_data[,predictors],cl=train_data[,response],k=3)
  confusion_matrix = table(Actual=test_data[,response],Predicted=knn_predictions)
  correct_classification = sum(diag(confusion_matrix))/sum(confusion_matrix)
  missclassification_rate = 1-correct_classification
  missclassification_rates = c(missclassification_rates,missclassification_rate)
}
sprintf("The misclassification rate for k folds are: %s",paste(missclassification_rates,collapse = ","))
plot(1:10, missclassification_rates, type = "b",pch =16,col="red", xlab = "K", ylab = "Misclassification Rate", main = "Misclassification rate for k folds")
average_missclassification = mean(missclassification_rates)
sprintf("The average misclassification rate after validation is %s",average_missclassification)

```

After the cross validation for the knn model with k =4 , the average misclassification rate comes out to be 0.04 i.e 4%

## Random Forest

### 50 Trees

```{r}
set.seed(1)
k=10
mis.errors.50=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)

for (i in 1:k){
  # split into train/test
  Iris.test=Iris[Iris.splits[[i]],]
  Iris.train=Iris[-Iris.splits[[i]],]
  
  # train model
  rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=50) 
  
  # predict on test and get error from confusion matrix
  predictions=predict(rf.fit, Iris.test)
  rf.cm=table(predictions,Iris.test$Class)
  mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
  
  mis.errors.50[i]=mis.error
}

mean(mis.errors.50)
```

### 100 Trees

```{r}
set.seed(1)
k=10
mis.errors.100=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)

for (i in 1:k){
  # split into train/test
  Iris.test=Iris[Iris.splits[[i]],]
  Iris.train=Iris[-Iris.splits[[i]],]
  
  # train model
  rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=100) 
  
  # predict on test and get error from confusion matrix
  predictions=predict(rf.fit, Iris.test)
  rf.cm=table(predictions,Iris.test$Class)
  mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
  
  mis.errors.100[i]=mis.error
}

mean(mis.errors.100)
```

### 500 Trees

```{r}
set.seed(1)
k=10
mis.errors.500=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)

for (i in 1:k){
  # split into train/test
  Iris.test=Iris[Iris.splits[[i]],]
  Iris.train=Iris[-Iris.splits[[i]],]
  
  # train model
  rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=500) 
  
  # predict on test and get error from confusion matrix
  predictions=predict(rf.fit, Iris.test)
  rf.cm=table(predictions,Iris.test$Class)
  mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
  
  mis.errors.500[i]=mis.error
}

mean(mis.errors.500)
```

### 1000 Trees

```{r}
set.seed(1)
k=10
mis.errors.1000=rep(k)
Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)

for (i in 1:k){
  # split into train/test
  Iris.test=Iris[Iris.splits[[i]],]
  Iris.train=Iris[-Iris.splits[[i]],]
  
  # train model
  rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=1000) 
  
  # predict on test and get error from confusion matrix
  predictions=predict(rf.fit, Iris.test)
  rf.cm=table(predictions,Iris.test$Class)
  mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
  
  mis.errors.1000[i]=mis.error
}

mean(mis.errors.1000)
```

### Plot of Estimated Test Error Rates (ntree=50,100,500,1000)

```{r}
ave.errors=c(mean(mis.errors.50),mean(mis.errors.100),mean(mis.errors.500),mean(mis.errors.1000))

plot(c(50,100,500,1000),ave.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='b')

title('Amount of Trees vs Test Error Rate')
```

### 1-100 Trees and Plot

After validating Random Forest models with 50, 100, 500, and 1000 trees, we found that 500 trees minimizes the estimated test error rate at 4%. However, these values were arbitrarily chosen, so lower error rates may occur between these values. I have chosen to further investigate every model between 1 and 100 trees.

```{r}
set.seed(1)
ntree.max=100
k=10

n.errors=rep(ntree.max) # store avg error for each model

# cross-validate each model with increasing ntree
for (n in 1:ntree.max){
  k.errors=rep(k)
  Iris.splits=createFolds(Iris$Class, k = k, returnTrain = FALSE)
  
  for (i in 1:k){
    # split into train/test
    Iris.test=Iris[Iris.splits[[i]],]
    Iris.train=Iris[-Iris.splits[[i]],]
    
    # train model
    rf.fit = randomForest(Iris.train$Class ~ ., data=Iris.train, type='classification',importance=TRUE, proximity=TRUE, ntree=n) 
    
    # predict on test and get error from confusion matrix
    predictions=predict(rf.fit, Iris.test)
    rf.cm=table(predictions,Iris.test$Class)
    mis.error=1-sum(diag(rf.cm))/sum(rf.cm)
    
    k.errors[i]=mis.error
  }
  n.errors[n]=mean(k.errors)
}
```

```{r}
error.min=min(n.errors)
error.min.index=which.min(n.errors)
error.min
error.min.index

plot(seq(1:ntree.max),n.errors, xlab = 'n trees', ylab='Estimated Test Error Rate', type='l')
title('Amount of Trees vs Test Error Rate')
```

Interestingly, the new minimum error rate is [3.33% when ntree=3]{.underline}.

## Conclusion

The estimated test misclassification error rates for each model were:

[**LDA - 2%**]{.underline}

[**QDA - 2%**]{.underline}

**KNN - 4%**

**Random Forest - 3%**

While every model performed well, the LDA and QDA models had the smallest error rates, 2%. The classes were easily separable (plotted below), meaning a linear function was sufficient for effective decision boundaries. This also explains why KNN did not perform as well, since KNN tends to dominate LDA when the decision boundary is highly non-linear. We suspect that Random Forest did not perform nearly as well since the dataset had both a small amount of predictors and observations. This resulted in fluctuating error depending on amount of trees used and predictors randomly chosen.

2 Dimensional Scatter Plots based on the classes they belong to.

```{r}
ggplot(data = iris,aes(x=Sepal.Length,y=Sepal.Width,col=Class))+geom_point()
```

```{r}
ggplot(data = iris,aes(x=Sepal.Width,y=Petal.Length,col=Class))+geom_point()
```

```{r}
ggplot(data = iris,aes(x=Petal.Width,y=Petal.Length,col=Class))+geom_point()
```

```{r}
ggplot(data = iris,aes(x=Petal.Width,y=Sepal.Length,col=Class))+geom_point()
```

```{r}
ggplot(data = iris,aes(x=Petal.Length,y=Sepal.Length,col=Class))+geom_point()
```

```{r}
ggplot(data = iris,aes(x=Petal.Width,y=Sepal.Width,col=Class))+geom_point()
```
